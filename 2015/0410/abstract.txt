The efficiency, and even feasibility, of large scale statistical learning is highly dependent of the computational complexity of the used underlying optimization algorithm. Traditional optimization techniques do not scale linearly with the sample size. This project aims to bring the Stochastic Average Gradient(SAG), in Le Roux, Schmidt, Bach (2014), to R. SAG is a large scale learning algorithm that, in the case of strongly convex cost functions, scales linearly with sample size.